# Image-to-Translation Project

Dá»± Ã¡n dá»‹ch text tá»« áº£nh tiáº¿ng Anh sang tiáº¿ng Viá»‡t sá»­ dá»¥ng Vision Transformer vÃ  Transformer Decoder.

## Kiáº¿n trÃºc

- **Vision Encoder**: Vision Transformer (ViT) Ä‘á»ƒ extract features tá»« áº£nh chá»©a text
- **Text Decoder**: Transformer decoder vá»›i cross-attention Ä‘á»ƒ generate text tiáº¿ng Viá»‡t
- **Cross-Attention**: Káº¿t ná»‘i giá»¯a vision features vÃ  text generation

## CÃ i Ä‘áº·t

```bash
# Install requirements
pip install torch torchvision torchtext
pip install underthesea  # Vietnamese tokenizer
pip install spacy  # English tokenizer (optional)
pip install pillow  # Image processing
pip install pandas  # Data handling

# Test installation
python test_project.py
```

## Chuáº©n bá»‹ dá»¯ liá»‡u

Dá»¯ liá»‡u cáº§n cÃ³ format JSON:

```json
[
  {
    "id": "00001",
    "image_path": "data/train/images/00001.png",
    "source_text": "Hello World",
    "target_text": "Xin chÃ o tháº¿ giá»›i"
  },
  {
    "id": "00002", 
    "image_path": "data/train/images/00002.png",
    "source_text": "Good morning",
    "target_text": "ChÃ o buá»•i sÃ¡ng"
  }
]
```

**Cáº¥u trÃºc thÆ° má»¥c:**
```
data/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ train.json
â”‚   â””â”€â”€ images/
â”‚       â”œâ”€â”€ 00001.png
â”‚       â””â”€â”€ 00002.png
â””â”€â”€ val/
    â”œâ”€â”€ val.json
    â””â”€â”€ images/
        â”œâ”€â”€ 10001.png
        â””â”€â”€ 10002.png
```

## Training

### Option 1: Normal Training (Requires powerful GPU)
```bash
python train.py -d /path/to/data -m /path/to/model_output
```

### Option 2: Incremental Training (For limited GPU memory)
```bash
# Check your system and optimize config first
python check_memory.py

# Automatic incremental training (recommended)
python incremental_train.py -d /path/to/data -m /path/to/model_output

# Manual incremental training with custom settings
python incremental_train.py -d /path/to/data -m /path/to/model_output -e 3 -mem 4
```

### Option 3: Resume Training
```bash
# Resume from latest checkpoint
python train.py -d /path/to/data -m /path/to/model_output -r latest

# Resume from specific checkpoint
python train.py -d /path/to/data -m /path/to/model_output -r /path/to/checkpoint.pt
```

### Memory Optimization Features

ğŸ§  **Automatic Memory Management:**
- **Mixed Precision Training**: Reduces memory usage by ~40%
- **Gradient Accumulation**: Simulate large batch size with smaller micro-batches
- **Checkpoint Management**: Auto-save and resume training
- **Memory Cleanup**: Automatic GPU memory cleanup between sessions

ğŸ“Š **Config Auto-Optimization:**
- **2-4GB GPU**: `d_model=256, batch_size=2, micro_batch=1`
- **4-8GB GPU**: `d_model=384, batch_size=4, micro_batch=2`  
- **8-12GB GPU**: `d_model=512, batch_size=6, micro_batch=3`
- **12GB+ GPU**: `d_model=512, batch_size=8, micro_batch=4`

**Tham sá»‘ config trong `config/config.json`:**
- `d_model`: 512/384/256 - Dimension cá»§a transformer (tÃ¹y GPU memory)
- `batch_size`: 8/4/2 - Effective batch size 
- `micro_batch_size`: 4/2/1 - Actual batch size per forward pass
- `gradient_accumulation_steps`: 2/4 - Steps to accumulate gradients
- `max_strlen`: 128 - Äá»™ dÃ i tá»‘i Ä‘a cá»§a cÃ¢u output
- `epoch`: 50 - Sá»‘ epoch training
- `n_layers`: 6/4 - Sá»‘ layer cá»§a transformer
- `heads`: 8/4 - Sá»‘ attention heads
- `image_size`: 224 - KÃ­ch thÆ°á»›c áº£nh input
- `mixed_precision`: true - Sá»­ dá»¥ng FP16 Ä‘á»ƒ tiáº¿t kiá»‡m memory
- `checkpoint_every_n_steps`: 500 - Tá»± Ä‘á»™ng save checkpoint má»—i N steps

## Inference

```bash
python inference.py -p /path/to/image.jpg -m /path/to/trained_model
```

## Cáº¥u trÃºc dá»± Ã¡n

```
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.json          # Cáº¥u hÃ¬nh model vá»›i memory optimization
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ transformer.py       # Main model vá»›i cross-attention
â”‚   â”œâ”€â”€ encoder.py           # Vision Transformer encoder
â”‚   â”œâ”€â”€ decoder.py           # Text decoder
â”‚   â””â”€â”€ ...                  # CÃ¡c module khÃ¡c
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ dataset.py           # Data loading vÃ  preprocessing
â”‚   â”œâ”€â”€ translator.py        # Inference pipeline
â”‚   â”œâ”€â”€ beam_search.py       # Beam search algorithm
â”‚   â”œâ”€â”€ tokenizer.py         # Vietnamese tokenizer
â”‚   â”œâ”€â”€ image_processor.py   # Image preprocessing
â”‚   â”œâ”€â”€ checkpoint_manager.py # Training checkpoint management
â”‚   â””â”€â”€ ...                  # CÃ¡c utility khÃ¡c
â”œâ”€â”€ train.py                 # Training script vá»›i resume capability
â”œâ”€â”€ incremental_train.py     # Incremental training for limited memory
â”œâ”€â”€ check_memory.py          # Memory checker vÃ  config optimizer
â”œâ”€â”€ inference.py             # Inference script
â””â”€â”€ test_project.py          # Test project setup
```

## LÆ°u Ã½ quan trá»ng

### Vá» dá»¯ liá»‡u:
- áº¢nh nÃªn cÃ³ text rÃµ rÃ ng, khÃ´ng bá»‹ má» hoáº·c xoay
- Text trong áº£nh nÃªn Ä‘Æ¡n giáº£n, khÃ´ng quÃ¡ phá»©c táº¡p
- Báº£n dá»‹ch tiáº¿ng Viá»‡t cáº§n chÃ­nh xÃ¡c vÃ  tá»± nhiÃªn

### Vá» training:
- **GPU yáº¿u (2-6GB)**: DÃ¹ng `incremental_train.py` Ä‘á»ƒ train tá»«ng pháº§n
- **GPU máº¡nh (8GB+)**: CÃ³ thá»ƒ dÃ¹ng `train.py` bÃ¬nh thÆ°á»ng
- Batch size cÃ³ thá»ƒ cáº§n giáº£m náº¿u thiáº¿u memory
- Training time: 10-50 epochs tÃ¹y vÃ o kÃ­ch thÆ°á»›c dataset
- **Checkpoint**: Tá»± Ä‘á»™ng save má»—i 500 steps, cÃ³ thá»ƒ resume báº¥t cá»© lÃºc nÃ o

### Vá» inference:
- Model load má»™t láº§n, cÃ³ thá»ƒ dá»‹ch nhiá»u áº£nh
- Beam search vá»›i k=5 cho káº¿t quáº£ tá»‘t
- Output sáº½ Ä‘Æ°á»£c post-process Ä‘á»ƒ clean punctuation

## VÃ­ dá»¥ sá»­ dá»¥ng

```python
# Test model
from utils.translator import translate
from utils.image_processor import ImageProcessor
import torch

# Load model (see inference.py for full example)
translation = translate(
    image_path="test_image.jpg", 
    model=model,
    vocab=vocab,
    max_strlen=128,
    device=device,
    k=5,
    image_processor=image_processor
)

print(f"Translation: {translation}")
```

## Troubleshooting

1. **Lá»—i tokenizer**: CÃ i Ä‘áº·t `pip install underthesea`
2. **Lá»—i CUDA/Memory**: 
   - Cháº¡y `python check_memory.py` Ä‘á»ƒ check GPU
   - DÃ¹ng `python incremental_train.py` cho GPU yáº¿u
   - Giáº£m `batch_size` vÃ  `d_model` trong config
3. **Training bá»‹ giÃ¡n Ä‘oáº¡n**: 
   - Resume vá»›i `python train.py -r latest`
   - Hoáº·c dÃ¹ng `incremental_train.py` Ä‘á»ƒ tá»± Ä‘á»™ng resume
4. **Lá»—i image loading**: Kiá»ƒm tra Ä‘Æ°á»ng dáº«n áº£nh trong JSON
5. **Káº¿t quáº£ dá»‹ch kÃ©m**: TÄƒng epoch training hoáº·c cáº£i thiá»‡n dataset
6. **Out of Memory**: 
   - Giáº£m `micro_batch_size` trong config
   - TÄƒng `gradient_accumulation_steps`
   - Báº­t `mixed_precision: true`
