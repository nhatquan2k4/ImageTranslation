# Decoder Transformer

TÃ i liá»‡u nÃ y trÃ¬nh bÃ y chi tiáº¿t vá» Decoder trong kiáº¿n trÃºc Transformer, bao gá»“m:
- Kiáº¿n trÃºc lá»›p (Masked Self-Attention, Cross-Attention, FFN)
- Masking (causal/look-ahead, padding, káº¿t há»£p mask)
- Má»¥c tiÃªu huáº¥n luyá»‡n (teacher forcing, shift-right, label smoothing)
- Suy luáº­n (greedy, beam search, sampling), KV cache
- Positional encodings (Absolute/Relative/RoPE/ALiBi)


---

<img src="img/decoder.jfif" alt="decoder picture">

---

## 1) Tá»•ng quan Decoder Transformer

- Má»¥c Ä‘Ã­ch: sinh ra mÃ£ chuá»—i Ä‘Ã­ch Y theo kiá»ƒu tá»± há»“i quy (autoregressive) Ä‘á»“ng thá»i liÃªn tá»¥c káº¿t há»£p ngá»¯ cáº£nh tá»« thÃ´ng tin mÃ£ hÃ³a cá»§a encoder (cÃ¡c token Ä‘Ã£ sinh).
- á»¨ng dá»¥ng:
  - Dá»‹ch mÃ¡y, tÃ³m táº¯t, sinh vÄƒn báº£n, há»™i thoáº¡i, sinh mÃ£, cÃ¡c tÃ¡c vá»¥ Ä‘iá»u kiá»‡n theo Ä‘áº§u vÃ o (vá»›i encoder) hoáº·c khÃ´ng (decoder-only).

---

## 2) Kiáº¿n trÃºc má»™t Decoder Layer 
<img src="img/decoder1.jfif">

- Má»—i decoder block cÃ³ cáº¥u trÃºc nhÆ° sau:

<img src="img/decoder2.jfif">

Gá»i ğ‘‹ lÃ  Ä‘áº§u vÃ o cá»§a khá»‘i Decoder (chuá»—i embedding tá»« bÆ°á»›c trÆ°á»›c hoáº·c tá»« embedding ban Ä‘áº§u cá»§a tá»« dá»‹ch).

Gá»i ğ» lÃ  output tá»« Encoder, nÃ³ cung cáº¥p ngá»¯ cáº£nh cá»§a cÃ¢u nguá»“n.
### 1. Self-Attention (Masked Multi-Head Attention)
$$
\tilde{X} = \text{LayerNorm}(X)
$$
$$
X' = X + \text{Attention}(\tilde{X}, \tilde{X}, \tilde{X})
$$

---

### 2. Cross-Attention (Encoderâ€“Decoder Attention)
$$
\tilde{X}' = \text{LayerNorm}(X')
$$
$$
X'' = X' + \text{Attention}(\tilde{X}', H, H)
$$

Trong Ä‘Ã³:  
- \(H\) lÃ  **encoder output** (ngá»¯ cáº£nh tá»« encoder).  
- \(X'\) lÃ  Ä‘áº§u ra sau self-attention.  

---

### 3. Feed Forward Network (FFN)
$$
\tilde{X}'' = \text{LayerNorm}(X'')
$$
$$
X''' = X'' + \text{FFN}(\tilde{X}'')
$$

---

### 4. Output
$$
Y = \text{LayerNorm}(X''')
$$

---

### CÃ´ng thá»©c gá»n cho toÃ n bá»™ Decoder block
$$
Y = \text{LayerNorm}\Big( X'' + \text{FFN}(\text{LayerNorm}(X'' )) \Big)
$$

Vá»›i:
$$
X'' = X' + \text{Attention}(\text{LayerNorm}(X'), H, H)
$$
$$
X' = X + \text{Attention}(\text{LayerNorm}(X), \text{LayerNorm}(X), \text{LayerNorm}(X))
$$

---

## 3) Self-Attention vÃ  Causal Mask

<img src="img/self_attention.jfif">

### Scaled Dot-Product Attention cho Self-Attention

1. TÃ­nh Q, K, V:
$$
Q = X W_Q, \quad K = X W_K, \quad V = X W_V
$$

2. TÃ­nh Ä‘iá»ƒm tÆ°Æ¡ng Ä‘á»“ng (scores):
$$
\text{Scores} = \frac{QK^T}{\sqrt{d_k}}, \quad \text{shape: } (B, h, T, T)
$$

3. Ãp dá»¥ng mask:
- **Causal mask** $(M_{\text{causal}}$) (tam giÃ¡c dÆ°á»›i):  
$$
M[i,j] = -\infty \quad \text{náº¿u } j > i
$$

- **Target padding mask** $(M_{\text{pad}}$):  
Che cÃ¡c vá»‹ trÃ­ bá»‹ padding (thÆ°á»ng che cá»™t \(j\) tÆ°Æ¡ng á»©ng token pad).

- Káº¿t há»£p:
$$
\text{Scores} = \text{Scores} + M_{\text{causal}} + M_{\text{pad}}
$$

4. TÃ­nh Attention:
$$
A = \text{softmax}(\text{Scores})
$$

5. Äáº§u ra:
$$
\text{Out} = A V
$$

---

**Trá»±c giÃ¡c**: á» thá»i Ä‘iá»ƒm \(t\), má»™t token chá»‰ Ä‘Æ°á»£c "nhÃ¬n" cÃ¡c token á»Ÿ vá»‹ trÃ­ $(\leq t$).


---

## 4) Cross-Attention (Encoder-Decoder Attention)

<img src="img/cross_attention.jfif">

### Scaled Dot-Product Attention cho Cross-Attention (Encoderâ€“Decoder Attention)

1. TÃ­nh Q, K, V:
$$
Q = X_{\text{dec}} W_Q, \quad K = H_{\text{enc}} W_K, \quad V = H_{\text{enc}} W_V
$$

2. TÃ­nh Ä‘iá»ƒm tÆ°Æ¡ng Ä‘á»“ng (scores):
$$
\text{Scores} = \frac{QK^T}{\sqrt{d_k}}, \quad \text{shape: } (B, h, T_{\text{tgt}}, S_{\text{src}})
$$

- $(B$): batch size  
- $(h$): sá»‘ head  
- $(T_{\text{tgt}}$): Ä‘á»™ dÃ i chuá»—i Ä‘Ã­ch (decoder input)  
- $(S_{\text{src}}$): Ä‘á»™ dÃ i chuá»—i nguá»“n (encoder input)  

3. Ãp dá»¥ng **encoder padding mask**:  
Che cÃ¡c cá»™t tÆ°Æ¡ng á»©ng vá»›i token bá»‹ padding trong cÃ¢u nguá»“n.

4. TÃ­nh Attention:
$$
A = \text{softmax}(\text{Scores})
$$

5. Äáº§u ra:
$$
\text{Out} = A V
$$

---

**Ã nghÄ©a**: Decoder truy xuáº¥t thÃ´ng tin ngá»¯ cáº£nh tá»« cÃ¢u nguá»“n Ä‘Ã£ Ä‘Æ°á»£c Encoder mÃ£ hÃ³a.

---

## 5) Má»¥c tiÃªu huáº¥n luyá»‡n (Teacher Forcing)

- Dá»‹ch trÃ¡i (shift-right): thÃªm token $<bos>$ á»Ÿ Ä‘áº§u chuá»—i Ä‘Ã­ch, dá»‹ch toÃ n bá»™ 1 bÆ°á»›c Ä‘á»ƒ dá»± Ä‘oÃ¡n token káº¿ tiáº¿p.
- Loss: cross-entropy giá»¯a logits á»Ÿ thá»i t vÃ  token tháº­t á»Ÿ t.
- Mask loss cho cÃ¡c vá»‹ trÃ­ $<pad>$.
- Label smoothing (vÃ­ dá»¥ Îµ=0.1) giÃºp tá»•ng quÃ¡t tá»‘t hÆ¡n (nháº¥t lÃ  seq2seq).
- ThÆ°á»ng â€œtied embeddingsâ€: dÃ¹ng chung ma tráº­n embedding vÃ  projection tá»›i vocab (giáº£m tham sá»‘, cáº£i thiá»‡n cháº¥t lÆ°á»£ng nháº¹).

---
